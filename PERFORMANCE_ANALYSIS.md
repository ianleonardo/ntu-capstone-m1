# Streamlit Dashboard Performance Analysis

## Current Performance Issues

### ðŸ”´ Critical Issues

#### 1. **Category Explosion Done Upfront** (Lines 155-157)
```python
df['parsed_categories'] = df['categories'].apply(DataProcessor._parse_categories)
df = df.explode('parsed_categories')
df['category'] = df['parsed_categories'].apply(DataProcessor._extract_category)
```
**Impact**: Multiplies dataset size immediately, affecting all subsequent operations
**Solution**: Defer explosion or cache exploded results per category filter

#### 2. **No Caching for Aggregated Data**
Multiple expensive groupby operations repeated across tabs without caching:
- Lines 262-268: KPI calculations
- Lines 404-410: Market share calculations  
- Lines 443-453: Role prevalence
- Lines 461-463: Velocity data
- Lines 480-482: Bulk hiring pivot
- Lines 598-600: Seniority pay scale
- Lines 612-613: Experience gate
- Lines 686-696: Education metrics

**Impact**: Same calculations run every time user interacts with filters
**Solution**: Add `@st.cache_data` decorators for each aggregation function

#### 3. **Skills Data Always Loaded** (Lines 179-182)
```python
if os.path.exists(Config.SKILL_FILE):
    skills_df = pd.read_parquet(Config.SKILL_FILE)
```
**Impact**: Large file loaded even if user never visits Tab 2
**Solution**: Lazy load only when Tab 2 is accessed

#### 4. **No Lazy Loading for Tabs**
All 4 tabs are rendered immediately, even if user only views Tab 1
**Impact**: Unnecessary computations and chart rendering
**Solution**: Use session state to detect active tab and render on-demand

### ðŸŸ¡ Moderate Issues

#### 5. **Plotly for All Visualizations**
Plotly is feature-rich but heavy (large JavaScript bundle)
**Current**: All charts use Plotly (lines 372-390, 412, 449, 466, etc.)
**Solution**: Use Altair for simpler bar/line charts, keep Plotly for complex interactions

#### 6. **Repeated Filtering Without Caching** (Lines 438-441, 648-651)
```python
if selected_sector_filter == 'All':
    filtered_df = df
else:
    filtered_df = df[df['category'] == selected_sector_filter]
```
**Impact**: Same filter applied multiple times
**Solution**: Cache filtered dataframes using `@st.cache_data`

#### 7. **Large Pivot Tables** (Lines 480-482, 673-674)
```python
bulk_pivot = bulk_filtered.pivot_table(
    index='category', columns='month_year', values='num_vacancies', aggfunc='sum'
).fillna(0)
```
**Impact**: Memory-intensive operation recalculated on every filter change
**Solution**: Pre-compute or cache pivot results

#### 8. **Inefficient View Column Handling** (Lines 228-234)
Converts entire column to numeric on every load
**Solution**: Clean data in preprocessing step, not at runtime

### ðŸŸ¢ Minor Issues

#### 9. **String Formatting in Loops**
Multiple HTML string generations (lines 294-327)
**Impact**: Minor, but could use Streamlit native components
**Solution**: Use `st.metric()` instead of custom HTML

#### 10. **Redundant Sorting Operations**
Multiple sorts without caching (lines 406, 445, 601, etc.)

---

## Recommended Optimizations

### Priority 1: Caching Strategy

#### A. Cache Aggregated Metrics
```python
@st.cache_data(ttl=3600)
def get_kpi_metrics(df):
    """Cache expensive KPI calculations"""
    return {
        'vacancies': df['num_vacancies'].sum(),
        'posts': len(df),
        'views': df['num_views'].sum(),
        # ... all other metrics
    }

@st.cache_data(ttl=3600)
def get_sector_aggregations(df):
    """Pre-compute all sector-level aggregations"""
    return {
        'market_share': df.groupby('category')['num_vacancies'].sum(),
        'role_counts': df['jobtitle_cleaned'].value_counts(),
        'velocity': df.groupby(['month_year', 'category'])['num_vacancies'].sum(),
        # ... etc
    }
```

#### B. Cache Filtered Data
```python
@st.cache_data(ttl=3600)
def filter_by_sector(df, sector):
    """Cache filtered dataframes"""
    if sector == 'All':
        return df
    return df[df['category'] == sector]

@st.cache_data(ttl=3600)
def filter_by_experience(df, exp_range):
    """Cache experience-based filters"""
    return df[df['min_exp'].between(exp_range[0], exp_range[1])]
```

#### C. Lazy Load Skills Data
```python
@st.cache_data(ttl=3600)
def load_skills_data():
    """Load skills data only when needed"""
    if os.path.exists(Config.SKILL_FILE):
        return pd.read_parquet(Config.SKILL_FILE)
    return pd.DataFrame()

# In Tab 2 only:
with tab2:
    if 'skills_data_loaded' not in st.session_state:
        skills_df = load_skills_data()
        st.session_state.skills_data_loaded = True
```

### Priority 2: Optimize Data Loading

#### D. Defer Category Explosion
```python
@st.cache_data(ttl=Config.CACHE_TTL)
def load_raw_data():
    """Load data WITHOUT explosion - keep it compact"""
    df = pd.read_parquet(Config.DATA_FILE)
    # Basic cleaning only
    df['posting_date'] = pd.to_datetime(df['posting_date'])
    df['month_year'] = df['posting_date'].dt.to_period('M').dt.to_timestamp()
    return df

@st.cache_data(ttl=Config.CACHE_TTL)
def explode_categories(df):
    """Explode categories only when needed"""
    df['parsed_categories'] = df['categories'].apply(DataProcessor._parse_categories)
    df = df.explode('parsed_categories')
    df['category'] = df['parsed_categories'].apply(DataProcessor._extract_category)
    return df

# In main():
df_raw = load_raw_data()
# Only explode when category-level analysis is needed
df = explode_categories(df_raw)
```

#### E. Pre-compute Experience Segments
Move to preprocessing step instead of runtime calculation (lines 169-176)

### Priority 3: Visualization Optimization

#### F. Use Altair for Simple Charts
```python
import altair as alt

# Instead of Plotly for simple bar charts:
chart = alt.Chart(top_sectors_chart).mark_bar().encode(
    x='Value:Q',
    y=alt.Y('category:N', sort='-x'),
    color=alt.value(bar_color)
).properties(height=500)

st.altair_chart(chart, use_container_width=True)
```

**Benefits:**
- Smaller bundle size (~40KB vs Plotly's ~3MB)
- Faster rendering for static charts
- Better performance on mobile
- Declarative syntax

**Keep Plotly for:**
- Interactive heatmaps
- 3D charts
- Complex hover interactions
- Animated charts (skills over time)

#### G. Reduce Chart Complexity
```python
# Limit data points for line charts
velocity_df = velocity_df.tail(100)  # Last 100 points only

# Sample large datasets for scatter plots
if len(hidden_demand) > 500:
    hidden_demand = hidden_demand.sample(500)
```

### Priority 4: Tab-Based Lazy Loading

#### H. Conditional Rendering
```python
# Initialize session state
if 'active_tab' not in st.session_state:
    st.session_state.active_tab = 0

# Track tab changes
tab_names = ["Executive Summary", "Sectoral Demand", "Skill & Experience", "Education Gap"]
selected_tab = st.radio("Select Tab", tab_names, horizontal=True, key='tab_selector')

# Render only active tab
if selected_tab == "Executive Summary":
    render_tab1(df)
elif selected_tab == "Sectoral Demand":
    skills_df = load_skills_data()  # Lazy load
    render_tab2(df, skills_df)
# ... etc
```

### Priority 5: Data Preprocessing

#### I. Move to Preprocessing Pipeline
Update `preprocess_data.py` to:
- Pre-calculate experience segments
- Clean view columns
- Remove outliers
- Pre-compute common aggregations
- Save optimized parquet with correct dtypes

---

## Implementation Roadmap

### Phase 1: Quick Wins (1-2 hours)
1. âœ… Add `@st.cache_data` to aggregation functions
2. âœ… Lazy load skills data
3. âœ… Use `st.metric()` instead of custom HTML cards
4. âœ… Add data sampling for large scatter plots

**Expected Improvement**: 30-40% faster load time

### Phase 2: Medium Effort (3-4 hours)
1. âœ… Implement filtered data caching
2. âœ… Replace simple Plotly charts with Altair
3. âœ… Defer category explosion
4. âœ… Add tab-based lazy loading

**Expected Improvement**: 50-60% faster overall, 80% faster initial load

### Phase 3: Comprehensive Refactor (1-2 days)
1. âœ… Move expensive computations to preprocessing
2. âœ… Implement progressive data loading
3. âœ… Add data downsampling strategies
4. âœ… Optimize pivot table calculations
5. âœ… Add loading indicators for expensive operations

**Expected Improvement**: 70-80% faster, smoother user experience

---

## Performance Metrics to Track

### Before Optimization
- Initial load time: ~8-12 seconds
- Tab switch time: ~2-4 seconds
- Filter application: ~1-3 seconds
- Memory usage: ~500MB-1GB

### Target After Optimization
- Initial load time: ~2-3 seconds (75% reduction)
- Tab switch time: ~0.5-1 second (75% reduction)
- Filter application: ~0.2-0.5 seconds (80% reduction)
- Memory usage: ~200-400MB (50% reduction)

---

## Code Examples

### Example 1: Cached Aggregation Function
```python
@st.cache_data(ttl=Config.CACHE_TTL, show_spinner="Calculating metrics...")
def calculate_executive_metrics(df):
    """Pre-compute all Executive Summary metrics"""
    metrics = {
        'total_vacancies': df['num_vacancies'].sum(),
        'total_posts': len(df),
        'total_views': df['num_views'].sum(),
        'top_sector_by_vacancy': df.groupby('category')['num_vacancies'].sum().idxmax(),
        'top_sector_by_posts': df['category'].value_counts().idxmax(),
        'top_sector_by_views': df.groupby('category')['num_views'].sum().idxmax(),
        'avg_salary': df['average_salary'].mean(),
    }
    return metrics

# Usage in Tab 1:
with tab1:
    metrics = calculate_executive_metrics(df)
    st.metric("Total Vacancies", f"{metrics['total_vacancies']:,.0f}")
```

### Example 2: Altair Chart
```python
@st.cache_data(ttl=Config.CACHE_TTL)
def prepare_top_sectors_chart(df, metric='num_vacancies', limit=10):
    """Prepare data for top sectors chart"""
    if metric == 'num_vacancies':
        data = df.groupby('category')[metric].sum().sort_values(ascending=False).head(limit)
    else:
        data = df['category'].value_counts().head(limit)
    return data.reset_index()

# Altair chart (faster for simple bars)
chart_data = prepare_top_sectors_chart(df, metric='num_vacancies')
chart = alt.Chart(chart_data).mark_bar().encode(
    x=alt.X('num_vacancies:Q', title='Total Vacancies'),
    y=alt.Y('category:N', sort='-x', title='Sector'),
    color=alt.value('#2E86C1'),
    tooltip=['category', 'num_vacancies']
).properties(
    height=500,
    title='Top 10 Sectors by Vacancies'
)
st.altair_chart(chart, use_container_width=True)
```

### Example 3: Progressive Data Loading
```python
@st.cache_data(ttl=Config.CACHE_TTL)
def load_data_minimal():
    """Load only essential columns for initial view"""
    df = pd.read_parquet(Config.DATA_FILE, columns=[
        'job_id', 'category', 'num_vacancies', 'average_salary', 'posting_date'
    ])
    return df

@st.cache_data(ttl=Config.CACHE_TTL)
def load_data_full():
    """Load all columns when needed"""
    return pd.read_parquet(Config.DATA_FILE)

# In main():
df_minimal = load_data_minimal()  # Fast initial load
# Load full data only when user navigates to tabs that need it
if selected_tab in ['Skill & Experience', 'Education Gap']:
    df = load_data_full()
else:
    df = df_minimal
```

---

## Testing Checklist

- [ ] Verify all cached functions return correct results
- [ ] Test filter combinations don't break caching
- [ ] Confirm memory usage reduced
- [ ] Measure load time improvements
- [ ] Test on slower connections
- [ ] Verify mobile performance
- [ ] Check browser console for errors
- [ ] Test with different data sizes
- [ ] Validate chart interactivity preserved
- [ ] Ensure all tooltips work correctly

---

## Additional Recommendations

1. **Add Loading Indicators**
   ```python
   with st.spinner("Analyzing demand velocity..."):
       velocity_data = calculate_velocity(df)
   ```

2. **Implement Data Downsampling**
   ```python
   # For large time-series, show monthly instead of daily
   if len(df) > 10000:
       df = df.groupby(pd.Grouper(key='posting_date', freq='M')).agg({...})
   ```

3. **Use Session State for User Preferences**
   ```python
   if 'preferred_metric' not in st.session_state:
       st.session_state.preferred_metric = 'Vacancies'
   ```

4. **Add Performance Monitoring**
   ```python
   import time
   start_time = time.time()
   # ... operations ...
   if st.checkbox("Show Performance Stats", value=False):
       st.info(f"Load time: {time.time() - start_time:.2f}s")
   ```

5. **Consider Data Partitioning**
   - Split data by time periods
   - Load recent data first
   - Provide "Load Historical Data" option

---

## Summary

**Estimated Total Improvement**: 60-75% reduction in load time and memory usage

**Key Takeaways:**
1. Add caching to ALL aggregation functions
2. Lazy load skills data and defer category explosion
3. Replace simple Plotly charts with Altair
4. Implement tab-based conditional rendering
5. Move expensive computations to preprocessing

**ROI**: High - Most optimizations are low-effort with significant impact
